//import org.apache.spark.sql.{SparkSession, DataFrame}
//import org.apache.spark.sql.functions._
//import org.apache.spark.sql.streaming.Trigger
//
//
////‚úÖ Reads streaming data from HDFS.
////‚úÖ Aggregates counts for each (event_time, animal).
////‚úÖ Uses UPSERT (MERGE INTO) to avoid duplicates in PostgreSQL.
////‚úÖ Uses foreachBatch() for efficient batch writes.
//
////üîπ How It Works
////1Ô∏è‚É£ Reads parquet files from HDFS incrementally using maxFilesPerTrigger.
////2Ô∏è‚É£ Groups data into 10-minute windows and counts events.
////3Ô∏è‚É£ Uses foreachBatch() to write each micro-batch to PostgreSQL.
////4Ô∏è‚É£ MERGE INTO (UPSERT) ensures:
////
////‚úÖ If event_time, animal already exists, update total_count.
////‚úÖ If new data appears, insert it as a new row.
////5Ô∏è‚É£ Uses Trigger.ProcessingTime("10 seconds") to process every 10 sec.
////üîπ Why This Approach?
////‚úÖ Efficient Streaming ‚Üí Processes micro-batches, not the entire dataset.
////‚úÖ No Data Loss ‚Üí Maintains previous counts while updating new ones.
////‚úÖ Prevents Duplicates ‚Üí Uses MERGE INTO to avoid duplicate entries.
////‚úÖ Fast Processing ‚Üí Uses parallelism & partitions for speed.
//
//class StreamingUpsertJob(spark: SparkSession) {
//
//  // ‚úÖ 1. Read Streaming Data from HDFS
//  def readStreamingData(): DataFrame = {
//    spark.readStream
//      .format("parquet")
//      .option("path", "hdfs://namenode:8020/data/events/")
//      .option("maxFilesPerTrigger", 10) // Process 10 files per micro-batch
//      .load()
//  }
//
//  // ‚úÖ 2. Aggregate Counts Per Window
//  def aggregateData(streamingDF: DataFrame): DataFrame = {
//    streamingDF
//      .withColumn("event_time", window(col("timestamp"), "10 minutes").start)
//      .groupBy("event_time", "animal")
//      .agg(count("*").alias("total_count"))
//  }
//
//  // ‚úÖ 3. UPSERT Function for PostgreSQL
//  def writeToPostgres(batchDF: DataFrame, batchId: Long): Unit = {
//    batchDF.createOrReplaceTempView("batch_data")
//
//    batchDF.sparkSession.sql(
//      """
//      MERGE INTO animal_counts AS target
//      USING batch_data AS source
//      ON target.event_time = source.event_time AND target.animal = source.animal
//      WHEN MATCHED THEN
//        UPDATE SET target.total_count = source.total_count
//      WHEN NOT MATCHED THEN
//        INSERT (event_time, animal, total_count)
//        VALUES (source.event_time, source.animal, source.total_count)
//      """
//    )
//  }
//
//  // ‚úÖ 4. Start Streaming Query
//  def start(): Unit = {
//    val streamingDF = readStreamingData()
//    val aggregatedDF = aggregateData(streamingDF)
//
//    val query = aggregatedDF.writeStream
//      .outputMode("update")  // Write only updated rows
//      .foreachBatch(writeToPostgres _)
//      .trigger(Trigger.ProcessingTime("10 seconds")) // Run every 10 sec
//      .start()
//
//    query.awaitTermination()
//  }
//}
//
//// ‚úÖ 5. Main Application Entry Point
//object StreamingUpsertApp {
//  def main(args: Array[String]): Unit = {
//    val spark = SparkSession.builder()
//      .appName("Streaming UPSERT to PostgreSQL")
//      .master("local[*]")
//      .config("spark.sql.shuffle.partitions", 200)
//      .getOrCreate()
//
//    val job = new StreamingUpsertJob(spark)
//    job.start()
//  }
//}
//
////Final PostgreSQL Table (animal_counts)
////Stored in PostgreSQL with UPSERT logic.
////
////üìå animal_counts (After First Batch)
////event_time	animal	total_count
////2024-03-07 12:00	dog	2
////2024-03-07 12:00	cat	2
////2024-03-07 12:10	dog	1
//
////üìå After Another Micro-Batch (If New Data Arrives)
////event_time	animal	total_count
////2024-03-07 12:00	dog	3
////2024-03-07 12:00	cat	2
////2024-03-07 12:10	dog	1
