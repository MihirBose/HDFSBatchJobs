//import org.apache.hadoop.fs.{FileSystem, Path}
//import org.apache.hadoop.conf.Configuration
//import org.apache.spark.sql.functions._
//import org.apache.spark.sql.types._
//import org.apache.spark.sql.{Dataset, Encoders, Row, SparkSession}
//import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout, Trigger}
//
////This Spark Structured Streaming job processes large parquet files in micro-batches using stateful aggregation and schema validation. It performs two different aggregations, joins them, and writes the final result to Parquet.
////
////ðŸ”¹ Key Features
////âœ… Schema Validation â†’ Ensures input data has the correct structure using Dataset[EventData].
////âœ… Incremental Processing â†’ Uses .option("maxFilesPerTrigger", 10) to process 10 files per batch.
////âœ… Stateful Aggregation â†’ Tracks user metrics (sessions, pageviews, duration, ad clicks) across micro-batches using GroupState.
////âœ… Secondary Aggregation â†’ Computes event type counts per user.
////âœ… Join Operation â†’ Merges both aggregated datasets on user_id.
////âœ… Efficient Output Writing â†’ Uses Update Mode, writing only updated records to Parquet.
////âœ… Optimized Processing â†’ Uses .trigger(Trigger.Once()) to write out the final result after all micro batches have been processed.
//
//
//object DailyDataAgg {
//
//  // âœ… Define case class for schema validation
//  case class EventData(
//                        user_id: String,
//                        session_id: String,
//                        pageview_id: String,
//                        session_duration: Long,
//                        event_type: String,
//                        event_time: String
//                      )
//
//  // âœ… Define case class for Aggregated Data
//  case class UserState(
//                        user_id: String,
//                        total_sessions: Long,
//                        total_pageviews: Long,
//                        total_duration: Long,
//                        ad_clicks: Long
//                      )
//
//  // âœ… Define case class for Per-User Event Type Count
//  case class EventTypeCount(
//                             user_id: String,
//                             event_type: String,
//                             event_count: Long
//                           )
//
//  def main(args: Array[String]): Unit = {
//
//    // âœ… Create Spark Session
//    val spark = SparkSession.builder()
//      .appName("Stateful Parquet Processing with Schema Validation")
//      .master("local[*]")
//      .getOrCreate()
//
//    import spark.implicits._
//
//    // âœ… Define Schema for CSV Data
//    val eventSchema = new StructType()
//      .add("user_id", StringType)
//      .add("session_id", StringType)
//      .add("pageview_id", StringType)
//      .add("session_duration", LongType)
//      .add("event_type", StringType)
//      .add("event_time", StringType)
//
//    // âœ… Step 1: Read Incrementally from CSV Files with Schema Validation
//    val rawData: Dataset[EventData] = spark.readStream
//      .format("csv")
//      .schema(eventSchema)  // âœ… Explicitly set schema
//      .option("header", "true")
//      .option("inferSchema", "true")
//      .option("path", "/Users/mihirbose/IdeaProjects/HDFSBatchJob/src/data/csv_source_data")
//      .option("maxFilesPerTrigger", 2)  // âœ… Read 2 files per micro-batch
//      .load()
//      .withColumn("event_time", col("event_time").cast(TimestampType)) // âœ… Ensure correct type
//      .as[EventData] // âœ… Enforce Schema Validation
//
//    // âœ… Step 2: Apply Window Function and Extract Start Time
//    val rawDataWithWindow = rawData
//      .withColumn("window", window(col("event_time"), "10 minutes")) // âœ… Create window struct
//      .withColumn("event_time", col("window.start")) // âœ… Extract start time
//      .drop("window") // âœ… Drop the struct if not needed
//
//    // âœ… Step 3: Define Stateful Aggregation Function
//    val updateStateFunc = (userId: String, batchData: Iterator[EventData], state: GroupState[UserState]) => {
//      val existingState = state.getOption.getOrElse(UserState(userId, 0, 0, 0, 0))
//
//      val batchTotalSessions = batchData.map(_.session_id).toSet.size.toLong
//      val batchTotalPageviews = batchData.size.toLong
//      val batchTotalDuration = batchData.map(_.session_duration).sum
//      val batchAdClicks = batchData.count(_.event_type == "ad_click")
//
//      val updatedState = UserState(
//        userId,
//        existingState.total_sessions + batchTotalSessions,
//        existingState.total_pageviews + batchTotalPageviews,
//        existingState.total_duration + batchTotalDuration,
//        existingState.ad_clicks + batchAdClicks
//      )
//
//      state.update(updatedState)
//      updatedState  // âœ… Return UserState instead of an Iterator
//    }
//
//
//    implicit val stringEncoder = Encoders.STRING // âœ… Encoder for user_id (String)
//    implicit val userStateEncoder = Encoders.product[UserState] // âœ… Encoder for UserState
//
//    // âœ… Step 4: Perform Stateful Aggregation
//    val userAggregates: Dataset[UserState] = rawDataWithWindow
//      .groupByKey(_.user_id)
//      .mapGroupsWithState(GroupStateTimeout.ProcessingTimeTimeout())(updateStateFunc)
//
//    // âœ… Step 5: Compute Event Type Counts
//    val eventTypeAggregates: Dataset[EventTypeCount] = rawDataWithWindow
//      .groupByKey(event => (event.user_id, event.event_type))
//      .count()
//      .map { case ((userId, eventType), count) => EventTypeCount(userId, eventType, count) }
//
//    // âœ… Step 6: Perform a Join Between Both Aggregations
//    val joinedData = userAggregates
//      .joinWith(eventTypeAggregates, userAggregates("user_id") === eventTypeAggregates("user_id"))
//
//    // âœ… Step 7: Write Final Result to Parquet
//    val query = rawDataWithWindow
//      .coalesce(1) // âœ… Force single output file
//      .writeStream
//      .outputMode("append")  // âœ… Only write updated rows
//      .format("parquet")
//      .option("path", "/Users/mihirbose/IdeaProjects/HDFSBatchJob/src/data/output_data")
//      .option("checkpointLocation", "/Users/mihirbose/IdeaProjects/HDFSBatchJob/src/data/checkpoints/")
//      .trigger(Trigger.ProcessingTime("1 seconds"))
//      .start()
//
//    query.awaitTermination(10000)
//  }
//}
